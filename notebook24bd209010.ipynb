{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57cd4b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:20:42.458144Z",
     "iopub.status.busy": "2024-08-29T19:20:42.457732Z",
     "iopub.status.idle": "2024-08-29T19:20:58.271430Z",
     "shell.execute_reply": "2024-08-29T19:20:58.270138Z"
    },
    "papermill": {
     "duration": 15.826931,
     "end_time": "2024-08-29T19:20:58.274264",
     "exception": false,
     "start_time": "2024-08-29T19:20:42.447333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: file:///kaggle/input/pytorch-metric-learning\r\n",
      "Processing /kaggle/input/pytorch-metric-learning/pytorch_metric_learning-2.6.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_metric_learning) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pytorch_metric_learning) (1.2.2)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_metric_learning) (2.1.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch_metric_learning) (4.66.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (2024.3.1)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch_metric_learning) (1.11.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch_metric_learning) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch_metric_learning) (3.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->pytorch_metric_learning) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->pytorch_metric_learning) (1.3.0)\r\n",
      "Installing collected packages: pytorch_metric_learning\r\n",
      "Successfully installed pytorch_metric_learning-2.6.0\r\n"
     ]
    }
   ],
   "source": [
    "# Als je een zip-bestand hebt geüpload, eerst uitpakken\n",
    "# !unzip -q /kaggle/input/pytorch-metric-learning.zip -d /kaggle/working/\n",
    "\n",
    "# Installeer de whl-bestanden\n",
    "# !pip install /kaggle/input/pytorch-metric-learning\n",
    "!pip install pytorch_metric_learning --no-index --find-links=file:///kaggle/input/pytorch-metric-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae479b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:20:58.294171Z",
     "iopub.status.busy": "2024-08-29T19:20:58.293766Z",
     "iopub.status.idle": "2024-08-29T19:21:06.120179Z",
     "shell.execute_reply": "2024-08-29T19:21:06.119267Z"
    },
    "papermill": {
     "duration": 7.83906,
     "end_time": "2024-08-29T19:21:06.122708",
     "exception": false,
     "start_time": "2024-08-29T19:20:58.283648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from pytorch_metric_learning import losses, miners\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from pytorch_metric_learning.distances import LpDistance\n",
    "from pytorch_metric_learning.miners import PairMarginMiner, TripletMarginMiner\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565ec63",
   "metadata": {
    "papermill": {
     "duration": 0.00842,
     "end_time": "2024-08-29T19:21:06.139829",
     "exception": false,
     "start_time": "2024-08-29T19:21:06.131409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415eb95b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:06.158813Z",
     "iopub.status.busy": "2024-08-29T19:21:06.157964Z",
     "iopub.status.idle": "2024-08-29T19:21:06.171462Z",
     "shell.execute_reply": "2024-08-29T19:21:06.170527Z"
    },
    "papermill": {
     "duration": 0.025481,
     "end_time": "2024-08-29T19:21:06.173675",
     "exception": false,
     "start_time": "2024-08-29T19:21:06.148194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"2024 ISIC Challenge primary prize scoring metric\n",
    "\n",
    "Given a list of binary labels, an associated list of prediction \n",
    "scores ranging from [0,1], this function produces, as a single value, \n",
    "the partial area under the receiver operating characteristic (pAUC) \n",
    "above a given true positive rate (TPR).\n",
    "https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "\n",
    "(c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80) -> float:\n",
    "    '''\n",
    "    2024 ISIC Challenge metric: pAUC\n",
    "    \n",
    "    Given a solution file and submission file, this function returns the\n",
    "    the partial area under the receiver operating characteristic (pAUC) \n",
    "    above a given true positive rate (TPR) = 0.80.\n",
    "    https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "    \n",
    "    (c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\n",
    "    Args:\n",
    "        solution: ground truth pd.DataFrame of 1s and 0s\n",
    "        submission: solution dataframe of predictions of scores ranging [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        Float value range [0, max_fpr]\n",
    "    '''\n",
    "\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "\n",
    "    # check submission is numeric\n",
    "    if not pd.api.types.is_numeric_dtype(submission.values):\n",
    "        raise ParticipantVisibleError('Submission target column must be numeric')\n",
    "\n",
    "    # rescale the target. set 0s to 1s and 1s to 0s (since sklearn only has max_fpr)\n",
    "    v_gt = abs(np.asarray(solution.values)-1)\n",
    "    \n",
    "    # flip the submissions to their compliments\n",
    "    v_pred = -1.0*np.asarray(submission.values)\n",
    "\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "\n",
    "    # using sklearn.metric functions: (1) roc_curve and (2) auc\n",
    "    fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n",
    "    if max_fpr is None or max_fpr == 1:\n",
    "        return auc(fpr, tpr)\n",
    "    if max_fpr <= 0 or max_fpr > 1:\n",
    "        raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n",
    "        \n",
    "    # Add a single point at max_fpr by linear interpolation\n",
    "    stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "    x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "    y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n",
    "    fpr = np.append(fpr[:stop], max_fpr)\n",
    "    partial_auc = auc(fpr, tpr)\n",
    "\n",
    "    #     # Equivalent code that uses sklearn's roc_auc_score\n",
    "    #     v_gt = abs(np.asarray(solution.values)-1)\n",
    "    #     v_pred = np.array([1.0 - x for x in submission.values])\n",
    "    #     max_fpr = abs(1-min_tpr)\n",
    "    #     partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    #     # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    #     # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    #     partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    \n",
    "    return(partial_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff767ed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:06.192420Z",
     "iopub.status.busy": "2024-08-29T19:21:06.192079Z",
     "iopub.status.idle": "2024-08-29T19:21:14.421488Z",
     "shell.execute_reply": "2024-08-29T19:21:14.420571Z"
    },
    "papermill": {
     "duration": 8.242028,
     "end_time": "2024-08-29T19:21:14.424353",
     "exception": false,
     "start_time": "2024-08-29T19:21:06.182325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/386316080.py:15: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_meta = pd.read_csv(os.path.join(data_path, 'train-metadata.csv'))\n"
     ]
    }
   ],
   "source": [
    "# # Load data\n",
    "data_path = '/kaggle/input/isic-2024-challenge/'\n",
    "path_train_hdf5 = os.path.join(data_path, 'train-image.hdf5')\n",
    "path_test_hdf5 = os.path.join(data_path, 'test-image.hdf5')\n",
    "\n",
    "path_train_meta = data_path + 'train-metadata.csv'\n",
    "path_test_meta = data_path + 'test-metadata.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Bestaande data\n",
    "data_path = '/kaggle/input/isic-2024-challenge/'\n",
    "path_train_hdf5 = os.path.join(data_path, 'train-image.hdf5')\n",
    "train_hdf5 = h5py.File(path_train_hdf5, 'r')\n",
    "train_meta = pd.read_csv(os.path.join(data_path, 'train-metadata.csv'))\n",
    "\n",
    "\n",
    "extra_dir_1 = '/kaggle/input/old-data/archive(4)/train-image/image/'\n",
    "extra_meta_1 = pd.read_csv('/kaggle/input/old-data/archive(4)/train-metadata.csv')\n",
    "\n",
    "extra_dir_2 = '/kaggle/input/old-data/archive(2)/train-image/image/'\n",
    "extra_meta_2 = pd.read_csv('/kaggle/input/old-data/archive(5)/train-metadata.csv')\n",
    "\n",
    "extra_dir_3 = '/kaggle/input/old-data/archive(3)/train-image/image/'\n",
    "extra_meta_3 = pd.read_csv('/kaggle/input/old-data/archive(3)/train-metadata.csv')\n",
    "\n",
    "# Combineer metadata (verondersteld dat image_id de kolomnaam is)\n",
    "combined_meta = pd.concat([train_meta, extra_meta_1, extra_meta_2, extra_meta_3], ignore_index=True)\n",
    "jpeg_dirs = [extra_dir_1, extra_dir_2, extra_dir_3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2384ea67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:14.444590Z",
     "iopub.status.busy": "2024-08-29T19:21:14.443871Z",
     "iopub.status.idle": "2024-08-29T19:21:14.453246Z",
     "shell.execute_reply": "2024-08-29T19:21:14.452275Z"
    },
    "papermill": {
     "duration": 0.021109,
     "end_time": "2024-08-29T19:21:14.455575",
     "exception": false,
     "start_time": "2024-08-29T19:21:14.434466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_hdf5 = h5py.File(path_train_hdf5, 'r')\n",
    "test_hdf5 = h5py.File(path_test_hdf5, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0473e31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:14.474320Z",
     "iopub.status.busy": "2024-08-29T19:21:14.473926Z",
     "iopub.status.idle": "2024-08-29T19:21:19.913165Z",
     "shell.execute_reply": "2024-08-29T19:21:19.911924Z"
    },
    "papermill": {
     "duration": 5.452415,
     "end_time": "2024-08-29T19:21:19.916683",
     "exception": false,
     "start_time": "2024-08-29T19:21:14.464268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/938936479.py:1: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_meta = pd.read_csv(path_train_meta)\n"
     ]
    }
   ],
   "source": [
    "train_meta = pd.read_csv(path_train_meta)\n",
    "test_meta = pd.read_csv(path_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564a826b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:19.939936Z",
     "iopub.status.busy": "2024-08-29T19:21:19.939560Z",
     "iopub.status.idle": "2024-08-29T19:21:20.046959Z",
     "shell.execute_reply": "2024-08-29T19:21:20.045718Z"
    },
    "papermill": {
     "duration": 0.121661,
     "end_time": "2024-08-29T19:21:20.050396",
     "exception": false,
     "start_time": "2024-08-29T19:21:19.928735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in the isic ids and target values\n",
    "train_isic_ids = train_meta['isic_id'].values\n",
    "train_isic_ids = train_meta[train_meta['lesion_id'].notnull()]['isic_id'].values\n",
    "\n",
    "test_isic_ids = test_meta['isic_id'].values\n",
    "\n",
    "train_targets = train_meta[train_meta['lesion_id'].notnull()]['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06598217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:20.077072Z",
     "iopub.status.busy": "2024-08-29T19:21:20.076023Z",
     "iopub.status.idle": "2024-08-29T19:21:20.084902Z",
     "shell.execute_reply": "2024-08-29T19:21:20.083711Z"
    },
    "papermill": {
     "duration": 0.023806,
     "end_time": "2024-08-29T19:21:20.087496",
     "exception": false,
     "start_time": "2024-08-29T19:21:20.063690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_size = len(train_targets)\n",
    "indices = np.arange(total_size)\n",
    "\n",
    "train_size = int(total_size * 0.8)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "train_indices, val_indices = train_test_split(indices, test_size=val_size, train_size=train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "213d82be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:20.107719Z",
     "iopub.status.busy": "2024-08-29T19:21:20.107312Z",
     "iopub.status.idle": "2024-08-29T19:21:20.112389Z",
     "shell.execute_reply": "2024-08-29T19:21:20.111183Z"
    },
    "papermill": {
     "duration": 0.018126,
     "end_time": "2024-08-29T19:21:20.114897",
     "exception": false,
     "start_time": "2024-08-29T19:21:20.096771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_samples = len(train_isic_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1be26357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:20.135874Z",
     "iopub.status.busy": "2024-08-29T19:21:20.134872Z",
     "iopub.status.idle": "2024-08-29T19:21:20.153477Z",
     "shell.execute_reply": "2024-08-29T19:21:20.152401Z"
    },
    "papermill": {
     "duration": 0.032003,
     "end_time": "2024-08-29T19:21:20.156296",
     "exception": false,
     "start_time": "2024-08-29T19:21:20.124293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Veronderstel dat je al de targets hebt (bijv. train_targets of labels)\n",
    "class_counts = np.bincount(train_targets)  # Aantal samples per klasse\n",
    "class_weights = 1. / class_counts  # Omgekeerde van het aantal samples per klasse\n",
    "sample_weights = class_weights[train_targets]  # Gewichten per sample gebaseerd op hun klasse\n",
    "\n",
    "# Maak een WeightedRandomSampler met deze sample gewichten\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=num_samples, replacement=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20761538",
   "metadata": {
    "papermill": {
     "duration": 0.009052,
     "end_time": "2024-08-29T19:21:20.174482",
     "exception": false,
     "start_time": "2024-08-29T19:21:20.165430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pytorch gedeelte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657fc5cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:20.195438Z",
     "iopub.status.busy": "2024-08-29T19:21:20.194948Z",
     "iopub.status.idle": "2024-08-29T19:21:33.057111Z",
     "shell.execute_reply": "2024-08-29T19:21:33.056237Z"
    },
    "papermill": {
     "duration": 12.876111,
     "end_time": "2024-08-29T19:21:33.059839",
     "exception": false,
     "start_time": "2024-08-29T19:21:20.183728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 19:21:23.150705: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-29 19:21:23.150819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-29 19:21:23.281478: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, ViTImageProcessor \n",
    "from torch import nn, optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "import random\n",
    "class TripletISICDataset(Dataset):\n",
    "    def __init__(self, hdf5_file=None, isic_ids=None, targets=None, jpeg_dirs=None, feature_extractor=None, transform=None):\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.isic_ids = isic_ids\n",
    "        self.targets = targets\n",
    "        self.jpeg_dirs = jpeg_dirs if jpeg_dirs is not None else []\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.isic_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        isic_id = str(self.isic_ids[idx])\n",
    "\n",
    "        # Load anchor image\n",
    "        anchor_image, anchor_label = self._load_image_and_label(idx)\n",
    "\n",
    "        # Get positive and negative ids based on implemented logic\n",
    "        positive_idx = self._get_positive_id(idx, anchor_label)\n",
    "        negative_idx = self._get_negative_id(idx, anchor_label)\n",
    "\n",
    "        # Load positive and negative images\n",
    "        positive_image, _ = self._load_image_and_label(positive_idx)\n",
    "        negative_image, _ = self._load_image_and_label(negative_idx)\n",
    "\n",
    "        return (anchor_image, positive_image, negative_image), anchor_label\n",
    "\n",
    "    def _get_positive_id(self, idx, anchor_label):\n",
    "        positive_indices = np.where(self.targets == anchor_label)[0]\n",
    "        positive_indices = positive_indices[positive_indices != idx]  # Vermijd dezelfde index\n",
    "        if len(positive_indices) == 0:\n",
    "            raise ValueError(\"Geen positieve sample gevonden\")\n",
    "        return np.random.choice(positive_indices)\n",
    "\n",
    "    def _get_negative_id(self, idx, anchor_label):\n",
    "        negative_indices = np.where(self.targets != anchor_label)[0]\n",
    "        if len(negative_indices) == 0:\n",
    "            raise ValueError(\"Geen negatieve sample gevonden\")\n",
    "        return np.random.choice(negative_indices)\n",
    "\n",
    "    def _load_image_and_label(self, idx):\n",
    "        isic_id = str(self.isic_ids[idx])\n",
    "        image = None\n",
    "\n",
    "        for jpeg_dir in self.jpeg_dirs:\n",
    "            image_path = os.path.join(jpeg_dir, f\"{isic_id}.jpeg\")\n",
    "            if os.path.exists(image_path):\n",
    "                image = Image.open(image_path)\n",
    "                break\n",
    "\n",
    "        if image is None and self.hdf5_file:\n",
    "            image = Image.open(BytesIO(self.hdf5_file[isic_id][()]))\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Afbeelding {isic_id} niet gevonden.\")\n",
    "\n",
    "        # Converteer de afbeelding naar RGB indien nodig\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "        # Toepassen van de transformaties\n",
    "        if self.feature_extractor:\n",
    "            image = self.feature_extractor(image)  # Zorg ervoor dat de extractor geen keyword 'images=' vereist\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(np.array(image)).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        label = self.targets[idx]\n",
    "        return image, label\n",
    "\n",
    "    def to_pytorch_loader(self, batch_size=32, shuffle=True, num_workers=4, sampler=None):\n",
    "        if sampler:\n",
    "            return DataLoader(self, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "        else:\n",
    "            return DataLoader(self, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        (anchor_images, positive_images, negative_images), labels = zip(*batch)\n",
    "        anchor_images = torch.stack(anchor_images)\n",
    "        positive_images = torch.stack(positive_images)\n",
    "        negative_images = torch.stack(negative_images)\n",
    "        labels = torch.tensor(labels)\n",
    "        return (anchor_images, positive_images, negative_images), labels\n",
    "\n",
    "# Stel de image transforms in voor ResNet-50\n",
    "feature_extractor = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7079bab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:33.080434Z",
     "iopub.status.busy": "2024-08-29T19:21:33.079424Z",
     "iopub.status.idle": "2024-08-29T19:21:35.243937Z",
     "shell.execute_reply": "2024-08-29T19:21:35.243046Z"
    },
    "papermill": {
     "duration": 2.177536,
     "end_time": "2024-08-29T19:21:35.246446",
     "exception": false,
     "start_time": "2024-08-29T19:21:33.068910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Laad het ResNet-50 model\n",
    "model = models.resnet50(pretrained=False)  # Zet pretrained=False omdat je eigen gewichten laadt\n",
    "# Pad naar de opgeslagen gewichten\n",
    "weights_path = \"/kaggle/input/resnet/pytorch/default/1/resnet50-0676ba61.pth\"\n",
    "\n",
    "# Laad de gewichten\n",
    "model.load_state_dict(torch.load(weights_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
    "\n",
    "# model = models.resnet50(\"/kaggle/input/resnet/pytorch/default/1\", pretrained=True)\n",
    "\n",
    "# Laad het ResNet-50 model en vervang de laatste volledig verbonden laag\n",
    "model.fc = nn.Linear(model.fc.in_features, 128)  # Pas de output aan naar 128 dimensies\n",
    "\n",
    "\n",
    "# Voor training met triplet loss, kan het helpen om de output te normaliseren\n",
    "class ResNet50TripletModel(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(ResNet50TripletModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Gebruik de backbone voor feature extractie\n",
    "        x = self.backbone(x)\n",
    "        # Normaliseer de embeddings\n",
    "        x = nn.functional.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "# Maak een instance van het aangepaste ResNet50 model\n",
    "triplet_model = ResNet50TripletModel(model)\n",
    "\n",
    "# Definieer de image transforms (feature extractor voor ResNet-50)\n",
    "# Stel de image transforms in voor ResNet-50\n",
    "feature_extractor = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Verplaats het model naar het juiste device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "triplet_model = triplet_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37e80be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:35.280179Z",
     "iopub.status.busy": "2024-08-29T19:21:35.279741Z",
     "iopub.status.idle": "2024-08-29T19:21:35.287162Z",
     "shell.execute_reply": "2024-08-29T19:21:35.286185Z"
    },
    "papermill": {
     "duration": 0.021951,
     "end_time": "2024-08-29T19:21:35.289461",
     "exception": false,
     "start_time": "2024-08-29T19:21:35.267510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class COMTripletLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(COMTripletLoss, self).__init__()\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Bepaal het centrum van de positieve klasse als het gemiddelde van de anker en positieve embeddings\n",
    "        center = (anchor + positive) / 2\n",
    "        \n",
    "        # Bereken de afstand van anchor en positive tot het centrum\n",
    "        pos_dist = (anchor - center).pow(2).sum(1) + (positive - center).pow(2).sum(1)\n",
    "        \n",
    "        # Bereken de afstand van negative tot het centrum\n",
    "        neg_dist = (negative - center).pow(2).sum(1)\n",
    "        \n",
    "        # De loss is de som van de positieve afstanden min de negatieve afstand\n",
    "        loss = pos_dist - neg_dist\n",
    "        \n",
    "        # Loss moet altijd positief zijn, daarom nemen we de gemiddelde waarde en nemen we de maximale waarde tussen loss en 0.\n",
    "        return loss.clamp(min=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdd27a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:35.308774Z",
     "iopub.status.busy": "2024-08-29T19:21:35.308183Z",
     "iopub.status.idle": "2024-08-29T19:21:35.316011Z",
     "shell.execute_reply": "2024-08-29T19:21:35.315191Z"
    },
    "papermill": {
     "duration": 0.019864,
     "end_time": "2024-08-29T19:21:35.318234",
     "exception": false,
     "start_time": "2024-08-29T19:21:35.298370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Loss functie en miner voor semi-hard negatieve mijnbouw\n",
    "# loss_func = losses.TripletMarginLoss(margin=0.2)\n",
    "\n",
    "# Loss functie en miner voor semi-hard negatieve mijnbouw\n",
    "loss_func = COMTripletLoss()\n",
    "\n",
    "miner = TripletMarginMiner(margin=0.05, type_of_triplets=\"semihard\")\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "035d5777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:35.337888Z",
     "iopub.status.busy": "2024-08-29T19:21:35.337537Z",
     "iopub.status.idle": "2024-08-29T19:21:35.465237Z",
     "shell.execute_reply": "2024-08-29T19:21:35.464284Z"
    },
    "papermill": {
     "duration": 0.140843,
     "end_time": "2024-08-29T19:21:35.468148",
     "exception": false,
     "start_time": "2024-08-29T19:21:35.327305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Maak een instance van de COM-Triplet Loss\n",
    "com_triplet_loss = COMTripletLoss()\n",
    "\n",
    "def train_com_triplet_model(model, dataloader, num_epochs=10, optimizer=None, loss_func=None, miner=None, scheduler=None, save_path=\"model_weights.pth\"):\n",
    "        model.train()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Unpack the triplets\n",
    "                (anchor_images, positive_images, negative_images), labels = batch\n",
    "\n",
    "                # Send to device\n",
    "                anchor_images = anchor_images.to(device)\n",
    "                positive_images = positive_images.to(device)\n",
    "                negative_images = negative_images.to(device)\n",
    "\n",
    "                # Compute embeddings\n",
    "                anchor_embedding = model(anchor_images)\n",
    "                positive_embedding = model(positive_images)\n",
    "                negative_embedding = model(negative_images)\n",
    "\n",
    "                embeddings = torch.cat([anchor_embedding, positive_embedding, negative_embedding], dim=0)\n",
    "                labels = torch.cat([labels, labels, labels])\n",
    "\n",
    "                # Mine hard negatives\n",
    "                hard_pairs = miner(embeddings, labels)\n",
    "\n",
    "                # Check if hard_pairs are empty\n",
    "                if any(t.numel() == 0 for t in hard_pairs):\n",
    "                    # Use a fallback loss calculation when no hard pairs are found\n",
    "                    loss = loss_func(anchor_embedding, positive_embedding, negative_embedding)\n",
    "                else:\n",
    "                    # Extract the indices for hard pairs\n",
    "                    anchor_idx, positive_idx, negative_idx = hard_pairs[:3]\n",
    "\n",
    "                    # Select only the hard pairs\n",
    "                    hard_anchor = embeddings[anchor_idx]\n",
    "                    hard_positive = embeddings[positive_idx]\n",
    "                    hard_negative = embeddings[negative_idx]\n",
    "\n",
    "                    # Compute loss with only hard pairs\n",
    "                    loss = loss_func(hard_anchor, hard_positive, hard_negative)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(dataloader):.4f}')\n",
    "\n",
    "        if save_path:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Model gewichten opgeslagen in {save_path}\")\n",
    "        \n",
    "# Creëer de dataset en dataloader\n",
    "triplet_dataset = TripletISICDataset(\n",
    "    hdf5_file=train_hdf5,\n",
    "    isic_ids=combined_meta[combined_meta['lesion_id'].notnull()]['isic_id'].values,\n",
    "    targets=combined_meta[combined_meta['lesion_id'].notnull()]['target'].values,\n",
    "    jpeg_dirs=jpeg_dirs,\n",
    "    feature_extractor=feature_extractor\n",
    ")\n",
    "train_dataloader = triplet_dataset.to_pytorch_loader(batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "# Start het training proces\n",
    "# train_com_triplet_model(model, train_dataloader, num_epochs=1, optimizer=optimizer, loss_func=loss_func, miner=miner, scheduler=scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e54134f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:35.488900Z",
     "iopub.status.busy": "2024-08-29T19:21:35.488231Z",
     "iopub.status.idle": "2024-08-29T19:21:37.042374Z",
     "shell.execute_reply": "2024-08-29T19:21:37.041237Z"
    },
    "papermill": {
     "duration": 1.567771,
     "end_time": "2024-08-29T19:21:37.045390",
     "exception": false,
     "start_time": "2024-08-29T19:21:35.477619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Laad het ResNet-50 model\n",
    "model = models.resnet50(pretrained=False)\n",
    "\n",
    "# Pas de laatste volledig verbonden laag aan voor het aantal outputfeatures (bijvoorbeeld 128)\n",
    "# Dit is belangrijk voor triplet loss waarbij de outputdimensie moet passen\n",
    "model.fc = nn.Linear(in_features=model.fc.in_features, out_features=128)\n",
    "\n",
    "# Laad de getrainde gewichten\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/triplet_loss_resnet/pytorch/default/1/model_weights.pth\"))\n",
    "model.eval()  # Zet het model in evaluatiemodus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68a852ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:37.067681Z",
     "iopub.status.busy": "2024-08-29T19:21:37.066832Z",
     "iopub.status.idle": "2024-08-29T19:21:37.073107Z",
     "shell.execute_reply": "2024-08-29T19:21:37.071960Z"
    },
    "papermill": {
     "duration": 0.020166,
     "end_time": "2024-08-29T19:21:37.075482",
     "exception": false,
     "start_time": "2024-08-29T19:21:37.055316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    images, ids = zip(*batch)  # Unzip de batch in images en ids\n",
    "    images = [img.unsqueeze(0) for img in images]  # Voeg een batch dim toe aan elke image\n",
    "    images = torch.cat(images, dim=0)  # Combineer alle images in één batch tensor\n",
    "    return images, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "788327bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:37.096963Z",
     "iopub.status.busy": "2024-08-29T19:21:37.096294Z",
     "iopub.status.idle": "2024-08-29T19:21:37.107498Z",
     "shell.execute_reply": "2024-08-29T19:21:37.106348Z"
    },
    "papermill": {
     "duration": 0.024559,
     "end_time": "2024-08-29T19:21:37.110035",
     "exception": false,
     "start_time": "2024-08-29T19:21:37.085476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Definieer de transform om alle afbeeldingen naar 224x224 te schalen en naar tensor om te zetten\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Schaal afbeeldingen naar 224x224 pixels\n",
    "    transforms.ToTensor(),          # Converteer naar een torch tensor\n",
    "])\n",
    "\n",
    "# Aangepaste datasetclass voor de testset\n",
    "class TestISICDataset(Dataset):\n",
    "    def __init__(self, hdf5_file, isic_ids, transform=None):\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.isic_ids = isic_ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.isic_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        isic_id = str(self.isic_ids[idx])\n",
    "        image = Image.open(BytesIO(self.hdf5_file[isic_id][()]))\n",
    "        image = image.convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(np.array(image)).permute(2, 0, 1).float() / 255.0\n",
    "        return image, isic_id\n",
    "\n",
    "# Laad de testset in een DataLoader met de aangepaste collate_fn en transform\n",
    "test_dataset = TestISICDataset(hdf5_file=test_hdf5, isic_ids=test_isic_ids, transform=image_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1ca3f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:37.131113Z",
     "iopub.status.busy": "2024-08-29T19:21:37.130702Z",
     "iopub.status.idle": "2024-08-29T19:21:38.097570Z",
     "shell.execute_reply": "2024-08-29T19:21:38.096182Z"
    },
    "papermill": {
     "duration": 0.980602,
     "end_time": "2024-08-29T19:21:38.100383",
     "exception": false,
     "start_time": "2024-08-29T19:21:37.119781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    }
   ],
   "source": [
    "def compute_embeddings(model, dataloader, device):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images, isic_ids = batch  # Zorg ervoor dat de batch correct opgesplitst wordt\n",
    "            \n",
    "            # Controleer of images een lijst is en stack alleen als dat nodig is\n",
    "            if isinstance(images, list):\n",
    "                images = torch.stack(images)  # Combineer de lijst van tensors tot één tensor\n",
    "            \n",
    "            images = images.to(device)  # Verplaats de tensor naar het apparaat\n",
    "            \n",
    "            # Bereken de embeddings met het model\n",
    "            embedding = model(images)\n",
    "            embeddings.append(embedding)\n",
    "            ids.extend(isic_ids)  # Voeg IDs toe aan de lijst\n",
    "    \n",
    "    embeddings = torch.cat(embeddings)  # Combineer alle embeddings tot één tensor\n",
    "    return embeddings, ids\n",
    "\n",
    "# Laad het getrainde model en zet het op het juiste device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Bereken embeddings voor de testset\n",
    "test_embeddings, test_ids = compute_embeddings(model, test_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "642b1097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T19:21:38.123600Z",
     "iopub.status.busy": "2024-08-29T19:21:38.123165Z",
     "iopub.status.idle": "2024-08-29T19:21:38.137863Z",
     "shell.execute_reply": "2024-08-29T19:21:38.136954Z"
    },
    "papermill": {
     "duration": 0.029589,
     "end_time": "2024-08-29T19:21:38.140397",
     "exception": false,
     "start_time": "2024-08-29T19:21:38.110808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Placeholder predictiefunctie\n",
    "def predict_labels(embeddings):\n",
    "    # Dummy implementatie - vervang met je daadwerkelijke predictielogica\n",
    "    return torch.zeros(len(embeddings))  # Vervang dit met daadwerkelijke voorspellingen\n",
    "\n",
    "# Voer voorspellingen uit op de test embeddings\n",
    "predicted_labels = predict_labels(test_embeddings)\n",
    "\n",
    "# Maak de submissie aan in een pandas DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'isic_id': test_ids,\n",
    "    'predicted_label': predicted_labels.cpu().numpy()  # Converteer tensor naar numpy array\n",
    "})\n",
    "\n",
    "# Opslaan van de submissie\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    },
    {
     "datasetId": 5575412,
     "sourceId": 9219508,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5614521,
     "sourceId": 9276546,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 94212,
     "modelInstanceId": 69085,
     "sourceId": 82227,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 94212,
     "modelInstanceId": 69085,
     "sourceId": 82335,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 111293,
     "modelInstanceId": 87054,
     "sourceId": 103851,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 111429,
     "modelInstanceId": 87190,
     "sourceId": 104012,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 62.412072,
   "end_time": "2024-08-29T19:21:41.643157",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-29T19:20:39.231085",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
